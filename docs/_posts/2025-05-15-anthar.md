---
layout: post
title:  "Evaluating AI Coding Agents: Introducing the ANTHAR Benchmark"
date:   2025-05-15 01:48:02 +0530
categories: deccan agents llms
---

Large Language Models are increasingly being used to help write and fix code. But how effective are they in real-world software engineering tasks? To answer this, we developed the **Deccan AI ANTHAR Benchmark** and used it to evaluate six popular autonomous coding agents: **Aider, SWE-agent, Cursor, OpenHands, Windsurf, and Devin.**

Our goal was to go beyond simple code generation and assess how these agents perform on challenges developers face daily.

## What is the ANTHAR Benchmark?

The ANTHAR benchmark is designed to simulate real-world software development scenarios.

*   **Real-world Tasks:** It consists of 40+ curated GitHub Pull Requests (PRs) from open-source Python and JavaScript repositories. These tasks cover bug fixes, feature additions, and code enhancements.
*   **Diverse Codebases:** Agents are tested on their ability to adapt to different project structures, libraries, and coding styles.
*   **Comprehensive Evaluation:** We combine automated testing (using the original repository's test suite) and security analysis with structured human evaluation.
*   **Multi-dimensional Assessment:** Agents were judged across several critical areas:
    *   **Correctness:** Does the code solve the problem and pass tests?
    *   **Helpfulness:** How much developer effort does the solution save?
    *   **Plan Creation:** How logical and sound is the agent's approach?
    *   **Plan Execution:** How well does the agent follow its plan?
    *   **File Localization:** Does the agent make changes in the correct files?
    *   **Security:** Does the code introduce vulnerabilities (checked using tools like Bandit for Python)?

## How We Tested

We recreated the state of a repository *before* the chosen GitHub PR was merged. We then gave the agents a clear prompt derived from the original issue or PR description. The original developer's solution (the PR) served as a "ground truth" reference.

We tested agents in two ways:
1.  **Pipeline-Compatible:** Agents like Aider, SWE-agent, and OpenHands were run in headless mode via scripts.
2.  **IDE-Integrated:** Agents like Cursor, Windsurf, and Devin required manual setup or supervision within their integrated development environments.

Automated tests and security scans were run on the agent's proposed changes. Human evaluators, experienced developers, then scored the agents based on the rubrics mentioned above.

## Key Findings

Our evaluation revealed significant insights into the current state of coding agents:

*   **Top Performers:**
    *   For **Python** problems, **Devin** achieved the highest correctness scores.
    *   For **JavaScript** problems, **Aider** led in correctness.
    *   **OpenHands** demonstrated strong overall performance across planning, execution, and alignment with the ground truth patches according to human evaluators.
*   **Qualitative Differences:** Human evaluation highlighted that while some agents had good *plans* (like Cursor), their actual code quality, build success, and alignment with the original PRs varied significantly. OpenHands generally performed best qualitatively, with Devin close behind. WindSurf often struggled with prompt satisfaction and code quality.
*   **Common Limitations Observed:**
    *   **Context Leakage:** IDE-based agents (Devin, Cursor, Windsurf) sometimes showed signs of picking up information from previous tasks, potentially leading to errors or privacy concerns.
    *   **Incomplete Solutions:** Agents often missed edge cases or failed to provide comprehensive test coverage.
    *   **Dependency Handling:** Managing project dependencies and ensuring correct integration remained a challenge for many.
    *   **Code Proliferation:** Agents sometimes introduced unnecessary complexity by modifying too many files or creating redundant components, potentially increasing technical debt.
    *   **Test Suite Dependence:** Agent performance evaluation heavily relies on the quality and comprehensiveness of the existing test suite.
*   **Agent Trends:** Devin appeared as a strong generalist leader in Python. Aider excelled in File Localization (knowing where to make changes). SWE-agent consistently underperformed compared to others in this benchmark.

<figure>
  <img src="/assets/image/python.png" alt="python image">
  <figcaption>fig 1: Modal Human Judgement Radar Chart — Python</figcaption>
</figure>

<figure>
  <img src="/assets/image/javascript.png" alt="python image">
  <figcaption>fig 2: Modal Human Judgement Radar Chart — Javascript</figcaption>
</figure>

## What This Means for Developers

Based on these findings, we have several recommendations:

*   **For Agent Developers:** Focus on generating code that is not only correct but also maintainable, modular, and aligns with existing codebase architecture. Implement safeguards against unnecessary file modifications and promote leveraging existing code components.
*   **For Software Engineers Using Agents:**
    *   **Choose Tasks Wisely:** Agents are currently best suited for well-scoped, routine tasks like simple bug fixes or basic UI updates. They struggle with complex architectural changes or tasks requiring deep system understanding.
    *   **Master Prompt Engineering:** Clear, explicit prompts detailing requirements, edge cases, and desired code quality significantly improve results. Requesting a plan first can also help.
    *   **Invest in Test Suites:** A robust test suite is crucial for validating agent outputs and safely integrating them into a project.

## Limitations

It's important to note the benchmark's current limitations, including its relatively modest sample size (43 tasks), the inherent subjectivity in human evaluation, and the challenge of using a single "ground truth" PR when alternative correct solutions might exist. We also did not analyze cost-efficiency (runtime, token usage) in this initial study.

## Conclusion

The ANTHAR benchmark provides valuable insights into the capabilities and limitations of current autonomous coding agents. While agents like Devin and Aider show impressive correctness in specific domains, and OpenHands demonstrates strong overall performance, no single agent is a universal solution. They offer significant promise for assisting developers with specific tasks but are not yet ready for full autonomy. Continued human oversight, careful task selection, robust prompting, and strong test coverage remain essential for effectively leveraging these powerful tools.

The ANTHAR benchmark serves as a foundation for future research to build more reliable, maintainable, and security-aware autonomous coding systems.
